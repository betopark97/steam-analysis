# Use the same Airflow base image as specified in your docker-compose.yml
# Ensure the version matches what you put in requirements.txt
FROM apache/airflow:3.0.1-python3.12

# Switch to root to install Docker client
USER root
RUN apt-get update && \
    apt-get install -y docker.io

# Switch back to the Airflow user
USER airflow

# Set the working directory inside the container
WORKDIR /opt/airflow

# Set the PYTHONPATH to include the Airflow directory
ENV PYTHONPATH="/opt/airflow:${PYTHONPATH}"

# Copy your custom requirements.txt into the container
COPY ./airflow_steam/requirements.txt .

# Install the Python dependencies using pip
# --no-cache-dir saves space by not caching package wheels
RUN pip install --no-cache-dir -r requirements.txt

# Copy data_pipeline/ and dbt_pg/ directories into the container
COPY ./data_pipeline/ /opt/airflow/data_pipeline/
COPY ./dbt_pg/ /opt/airflow/dbt_pg/