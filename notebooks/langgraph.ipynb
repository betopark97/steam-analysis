{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9454717b",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dd06ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System\n",
    "import os \n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Data Type\n",
    "import json\n",
    "from textwrap import dedent\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c1829f",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e32e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "print(\"Project root added to path.\")\n",
    "\n",
    "env_path = Path.cwd().parent / '.env'\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# Load environment variables\n",
    "print(f\"Environment variables loaded: {load_dotenv()}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91377c2a",
   "metadata": {},
   "source": [
    "# Gemini Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53300e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "model = 'gemini-2.5-flash-preview-04-17'\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    api_key=os.environ.get(\"GEMINI_API_KEY\"),\n",
    "    model=model, \n",
    "    temperature=0.6,\n",
    ")\n",
    "llm.invoke(\"Hello, testing connections!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de44ddc6",
   "metadata": {},
   "source": [
    "# PGVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb9b9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_postgres.vectorstores import PGVector\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/gemini-embedding-exp-03-07\",\n",
    "    google_api_key=os.environ.get(\"GEMINI_API_KEY\")\n",
    ")\n",
    "\n",
    "# See docker command above to launch a postgres instance with pgvector enabled.\n",
    "pg_user = os.environ.get('DB_USER')\n",
    "pg_password = os.environ.get('DB_PASSWORD')\n",
    "pg_db = os.environ.get('DB_NAME')\n",
    "pg_host = os.environ.get('DB_HOST')\n",
    "pg_port = os.environ.get('DB_PORT')\n",
    "connection = f\"postgresql+psycopg://{pg_user}:{pg_password}@{pg_host}:{pg_port}/{pg_db}\"\n",
    "collection_name = \"steam_games\"\n",
    "\n",
    "vector_store = PGVector(\n",
    "    embeddings=embeddings,\n",
    "    collection_name=collection_name,\n",
    "    connection=connection,\n",
    "    use_jsonb=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa37bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.query_constructor.schema import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "\n",
    "\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"name\",\n",
    "        description=\"The name of the game\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"year\",\n",
    "        description=\"The year the movie was released\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"director\",\n",
    "        description=\"The name of the movie director\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"rating\", description=\"A 1-10 rating for the movie\", type=\"float\"\n",
    "    ),\n",
    "]\n",
    "document_content_description = \"Brief summary of a movie\"\n",
    "llm = ChatGoogleGenerativeAI()\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm, vector_store, document_content_description, metadata_field_info, verbose=True\n",
    ")\n",
    "\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vector_store,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    enable_limit=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# This example only specifies a relevant query\n",
    "retriever.invoke(\"what are two movies about dinosaurs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada6f61e",
   "metadata": {},
   "source": [
    "# LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649cc780",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f49039",
   "metadata": {},
   "source": [
    "checkout: https://python.langchain.com/docs/integrations/tools/discord/  \n",
    "for discord tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1520fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "\n",
    "search = DuckDuckGoSearchResults(output_format='json')\n",
    "\n",
    "search.invoke(\"Obama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb11473",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from langchain_tavily import TavilySearch\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "\n",
    "@tool\n",
    "def tavily_search(query):\n",
    "    \"\"\"Returns search results from Tavily.\"\"\"\n",
    "    return TavilySearch(max_results=2).invoke(query)\n",
    "\n",
    "@tool\n",
    "def get_time():\n",
    "    \"\"\"Returns the current system time.\"\"\"\n",
    "    from datetime import datetime\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "\n",
    "tools = [tavily_search, get_time]\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28834818",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.agent_toolkits import SQLDatabaseToolkit\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "\n",
    "pg_uri = f\"postgresql+psycopg2://{pg_user}:{pg_password}@{pg_host}:{pg_port}/{pg_db}\"\n",
    "db = SQLDatabase.from_uri(pg_uri)\n",
    "\n",
    "print(f\"Dialect: {db.dialect}\")\n",
    "# print(f\"Available tables: {db.get_usable_table_names()}\")\n",
    "# print(f'Sample output: {db.run(\"SELECT * FROM bronze.details LIMIT 5;\")}')\n",
    "\n",
    "\n",
    "toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n",
    "\n",
    "tools = toolkit.get_tools()\n",
    "\n",
    "for tool in tools:\n",
    "    print(f\"{tool.name}: {tool.description}\\n\")\n",
    "    \n",
    "system_prompt = \"\"\"\n",
    "You are an agent designed to interact with a SQL database.\n",
    "Given an input question, create a syntactically correct {dialect} query to run,\n",
    "then look at the results of the query and return the answer. Unless the user\n",
    "specifies a specific number of examples they wish to obtain, always limit your\n",
    "query to at most {top_k} results.\n",
    "\n",
    "You can order the results by a relevant column to return the most interesting\n",
    "examples in the database. Never query for all the columns from a specific table,\n",
    "only ask for the relevant columns given the question.\n",
    "\n",
    "You MUST double check your query before executing it. If you get an error while\n",
    "executing a query, rewrite the query and try again.\n",
    "\n",
    "DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the\n",
    "database.\n",
    "\n",
    "To start you should ALWAYS look at the tables in the database to see what you\n",
    "can query. Do NOT skip this step.\n",
    "\n",
    "Then you should query the schema of the most relevant tables.\n",
    "\"\"\".format(\n",
    "    dialect=db.dialect,\n",
    "    top_k=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba922286",
   "metadata": {},
   "source": [
    "## Agent Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d51293d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# Create Agent\n",
    "agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=tools,\n",
    "    prompt=system_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6c0542",
   "metadata": {},
   "source": [
    "## State Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c2c8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph.message import add_messages\n",
    "from typing import TypedDict, Annotated, List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class State(TypedDict):\n",
    "    context: Annotated[List[Document], add_messages]\n",
    "    answer: Annotated[List[Document], add_messages]\n",
    "    question: Annotated[str, 'user question']\n",
    "    sql_query: Annotated[str, 'sql query']\n",
    "    binary_score: Annotated[str, 'binary score yes or no']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa90d1d2",
   "metadata": {},
   "source": [
    "## Node Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76b103f",
   "metadata": {},
   "source": [
    "all nodes have to be functions, the class GraphState will be passed to these functions but in the end these are just TypedDict so basically python dictionaries\n",
    "\n",
    "meaning that:  \n",
    "`return GraphState(context=documents) == {'context': documents}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10725ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state: State) -> State:\n",
    "    # retrieve: 검색\n",
    "    documents = \"검색된 문서\"\n",
    "    return {\"context\": documents}\n",
    "\n",
    "\n",
    "def rewrite_query(state: State) -> State:\n",
    "    # Query Transform: 쿼리 재작성\n",
    "    documents = \"검색된 문서\"\n",
    "    return State(context=documents)\n",
    "\n",
    "\n",
    "def execute_gemini(state: State) -> State:\n",
    "    # LLM 실행\n",
    "    answer = \"Gemini's Response\"\n",
    "    return State(answer=answer)\n",
    "\n",
    "\n",
    "def relevance_check(state: State) -> State:\n",
    "    # Relevance Check: 관련성 확인\n",
    "    binary_score = \"Relevance Score\"\n",
    "    return State(binary_score=binary_score)\n",
    "\n",
    "\n",
    "def sum_up(state: State) -> State:\n",
    "    # sum_up: 결과 종합\n",
    "    answer = \"종합된 답변\"\n",
    "    return State(answer=answer)\n",
    "\n",
    "\n",
    "def search_on_web(state: State) -> State:\n",
    "    # Search on Web: 웹 검색\n",
    "    documents = state[\"context\"] = \"기존 문서\"\n",
    "    searched_documents = \"검색된 문서\"\n",
    "    documents += searched_documents\n",
    "    return State(context=documents)\n",
    "\n",
    "\n",
    "def get_table_info(state: State) -> State:\n",
    "    # Get Table Info: 테이블 정보 가져오기\n",
    "    table_info = \"테이블 정보\"\n",
    "    return State(context=table_info)\n",
    "\n",
    "\n",
    "def generate_sql_query(state: State) -> State:\n",
    "    # Make SQL Query: SQL 쿼리 생성\n",
    "    sql_query = \"SQL 쿼리\"\n",
    "    return State(sql_query=sql_query)\n",
    "\n",
    "\n",
    "def execute_sql_query(state: State) -> State:\n",
    "    # Execute SQL Query: SQL 쿼리 실행\n",
    "    sql_result = \"SQL 결과\"\n",
    "    return State(context=sql_result)\n",
    "\n",
    "\n",
    "def validate_sql_query(state: State) -> State:\n",
    "    # Validate SQL Query: SQL 쿼리 검증\n",
    "    binary_score = \"SQL 쿼리 검증 결과\"\n",
    "    return State(binary_score=binary_score)\n",
    "\n",
    "\n",
    "def handle_error(state: State) -> State:\n",
    "    # Error Handling: 에러 처리\n",
    "    error = \"에러 발생\"\n",
    "    return State(context=error)\n",
    "\n",
    "\n",
    "def decision(state: State) -> State:\n",
    "    # 의사결정\n",
    "    decision = \"결정\"\n",
    "    # 로직을 추가할 수 가 있고요.\n",
    "\n",
    "    if state[\"binary_score\"] == \"yes\":\n",
    "        return \"종료\"\n",
    "    else:\n",
    "        return \"재검색\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fa60ac",
   "metadata": {},
   "source": [
    "## Graph Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e903c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangGraph\n",
    "from langgraph.graph import START,END, StateGraph\n",
    "\n",
    "# langgraph.graph에서 StateGraph와 END를 가져옵니다.\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# 노드를 추가합니다.\n",
    "workflow.add_node(\"query\", retrieve)\n",
    "workflow.add_node(\"rewrite_query\", rewrite_query)\n",
    "workflow.add_node(\"rewrite_question\", rewrite_query)\n",
    "workflow.add_node(\"execute_gemini\", execute_gemini)\n",
    "workflow.add_node(\"relevance_check_gemini\", relevance_check)\n",
    "workflow.add_node(\"결과 종합\", sum_up)\n",
    "workflow.add_node(\"get_table_info\", get_table_info)\n",
    "workflow.add_node(\"generate_sql_query\", generate_sql_query)\n",
    "workflow.add_node(\"execute_sql_query\", execute_sql_query)\n",
    "workflow.add_node(\"validate_sql_query\", validate_sql_query)\n",
    "\n",
    "# 각 노드들을 연결합니다.\n",
    "workflow.add_edge(START, \"query\")\n",
    "workflow.add_edge(\"query\", \"get_table_info\")\n",
    "workflow.add_edge(\"get_table_info\", \"generate_sql_query\")\n",
    "workflow.add_edge(\"generate_sql_query\", \"execute_sql_query\")\n",
    "workflow.add_edge(\"execute_sql_query\", \"validate_sql_query\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"validate_sql_query\",\n",
    "    decision,\n",
    "    {\n",
    "        \"QUERY ERROR\": \"rewrite_query\",\n",
    "        \"UNKNOWN MEANING\": \"rewrite_question\",\n",
    "        \"PASS\": \"execute_gemini\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"rewrite_query\", \"execute_sql_query\")\n",
    "workflow.add_edge(\"rewrite_question\", \"rewrite_query\")\n",
    "workflow.add_edge(\"execute_gemini\", \"relevance_check_gemini\")\n",
    "workflow.add_edge(\"relevance_check_gemini\", \"결과 종합\")\n",
    "workflow.add_edge(\"결과 종합\", END)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ae13bc",
   "metadata": {},
   "source": [
    "## Memory Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a7b05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# 기록을 위한 메모리 저장소를 설정합니다.\n",
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0bfcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from psycopg_pool import ConnectionPool\n",
    "from langgraph.checkpoint.postgres import PostgresSaver\n",
    "from psycopg.rows import dict_row\n",
    "\n",
    "conninfo = f\"postgres://{pg_user}:{pg_password}@{pg_host}:{pg_port}/{pg_db}?sslmode=disable\"\n",
    "connection_kwargs = {\n",
    "    \"autocommit\": True,\n",
    "    \"prepare_threshold\": 0,\n",
    "    \"row_factory\": dict_row\n",
    "}\n",
    "\n",
    "pool = ConnectionPool(\n",
    "    conninfo=conninfo,\n",
    "    max_size=20,\n",
    "    kwargs=connection_kwargs,\n",
    ")\n",
    "\n",
    "checkpointer = PostgresSaver(pool)\n",
    "\n",
    "checkpointer.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e29b56a",
   "metadata": {},
   "source": [
    "## Graph Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871ee3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프를 컴파일합니다.\n",
    "app = workflow.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ff1e6a",
   "metadata": {},
   "source": [
    "## Graph Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab42465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangGraph Visualization\n",
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph_mermaid import MermaidDrawMethod\n",
    "\n",
    "# Visualize\n",
    "Image(\n",
    "    app\n",
    "    .get_graph()\n",
    "    .draw_mermaid_png(\n",
    "        max_retries=5, retry_delay=5, draw_method=MermaidDrawMethod.API,\n",
    "        output_file_path='../reports/figures/langgraph_viz.png'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222eb748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd2d2a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
